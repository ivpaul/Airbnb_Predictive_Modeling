# -*- coding: utf-8 -*-
"""Group_545.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RgUIG_SqlyHU0VbCsj5OpW1_K7PxX76B

# **CIS 5450 Group Project: Predicting Airbnb Price**

**Problem to Solve:**
As a team of data scientists, our primary objective is to develop a robust predictive model that empowers Airbnb investors to optimize their pricing strategies effectively. By accurately forecasting revenue potential, we can provide valuable insights into monthly cash flow, enabling investors to make informed decisions and maximize profitability.

**Team Members:**
*   Ivan Paul Dingle (ivanpaul)
*   Peter Liu (yuhsul1)
*   Selasi Doe (selasi)
*   Tingting Duan(duant)

# **Imports/Setup**

Run the following cells to set up the notebook.
"""

# import packages
import pandas as pd
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
from sklearn.model_selection import train_test_split
import plotly.express as px
import json
from collections import Counter
import nltk
from nltk.corpus import stopwords
from textblob import TextBlob
import re
import datetime as dt
from wordcloud import WordCloud

"""#**Part 1: Data Loading and Preprocessing**

##**1.1 Data Loading**

We used one CSV file from Kaggle for this project, `Airbnb_Open_Data.csv`, which was stored into a dataframe `df_airbnb`. The dataset contains Airbnb properties located all throughout NYC. While the dataset contained features pertaining to location, neighbourhood, room types, house rules, and various credentials, we did not have information in regard to amenities, number of rooms/beds, or square footage.  We were most interested in the `price` column, a string variable indiciating a property's market price, to build a prediction model on future Airbnb properties. We also used an additional GeoJSON file from Kaggle to help identify NYC zipcodes for location coordinates.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !apt update
# !pip install kaggle
# !pip install uszipcode

# Run this cell to mount Google drive
from google.colab import drive
drive.mount('/content/drive')
# Create the kaggle directory
!mkdir ~/.kaggle

# Read the uploaded kaggle.json file
!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/

# Download dataset
!!kaggle datasets download -d arianazmoudeh/airbnbopendata

# Unzip folder in Colab content folder
!unzip /content/airbnbopendata.zip

'''TOGGLE HERE IF ALREADY RAN THROUGH 2.3.1 Geolocation'''
# df_airbnb = pd.read_csv('/content/drive/MyDrive/airbnb_clean.csv')

'''Don't run this cell if running the cell above'''
# Read the csv file and save it to a dataframe called "df_airbnb"
df_airbnb = pd.read_csv('Airbnb_Open_Data.csv')

# Geojson file for using plotly to define zipcode region
!!kaggle datasets download -d ashioyajotham/nyc-geojson-file

!unzip /content/nyc-geojson-file.zip

"""## **1.2 Data Preview**

We individually started to explore our dataset `df_airbnb` as a preview to form our own understandings on how to approach preprocessing in preparation for Exploratory Data Analysis (EDA).
"""

# Peek at the first three rows to understand the values in each column
df_airbnb.head(2)

# check shape
df_airbnb.shape

# check basic stat
df_airbnb.describe()

# check the datatypes in `df_airbnb`
df_airbnb.dtypes

"""## **1.3 Data Cleaning**

#### **1.3.1 Standardize Column Names**

For column names, we used standardized snake case to ensure consistency, which would aid in EDA and building models throughout the course of the project.
"""

# standardize column names
df_airbnb.columns = (df_airbnb.columns
                      .str.replace(' ', '_', regex=True)
                      .str.lower()
                    )
df_airbnb.columns

"""#### **1.3.2 Handle Duplicates**

We quanitified our number of duplicates in order to make a decision on how to clean our data.
"""

# find out how many duplicates we have
print(int(len(df_airbnb)-len(df_airbnb.drop_duplicates())))

# find out the % of dup of total records
int(len(df_airbnb)-len(df_airbnb.drop_duplicates()))/len(df_airbnb)*100

# manually check some duplicates
ids = df_airbnb["id"]
df_airbnb[ids.isin(ids[ids.duplicated()])].sort_values("id")[:2]

"""##### **Decision to Handle Duplicated Values**
- Upon checking, the duplicate values were all true duplicates across the entire row, not duplicates in one or two columns (e.g., not the same listing but with different prices). The number of duplicates account for about 0.5% of the entire dataset. Therefore, we collectively decided to drop all duplicates from `df_airbnb`.
"""

df_airbnb = df_airbnb.drop_duplicates()

"""#### **1.3.3 Handle Null**

We then quantified the number of null values in our dataset and discussed how to handle our data.
"""

# Find number of rows with null values
for col in df_airbnb.columns:
  print(f"{int(int(df_airbnb[col].isnull().sum())/len(df_airbnb[col])*100)}% of total rows; number of rows with null values in {col} is {int(df_airbnb[col].isnull().sum())}")

"""##### **Decision to Handle Null Values**
- Since the `license` column was 99% null, we decided it was safe to drop the entire column.
- If `last_review` and `reviews_per_month` are null, it was likely no one had reviewed it yet. Our strategy here was to fill `reviews_per_month` with 0 and `last_review` with the oldest date in this column.
- The `house_rules` column was 50% null. Because Airbnb has their own standard house rule policy for guests, not every property manager chose to add additional specified rules for their property.  We decided to use a place holder "no rule" for all the null values.
- The remaining columns, whose percentage of null values within our dataset were close to 0%, were dropped.
"""

# Drop the "license" column because 99% rows are null
df_airbnb.drop(columns=["license"], inplace=True)

# Fill "reviews_per_month" with 0 and "last_review" with the oldest date when null
df_airbnb["reviews_per_month"].fillna(0, inplace=True)

# Convert "last_review" column to datetime format
df_airbnb["last_review"] = pd.to_datetime(df_airbnb["last_review"])
df_airbnb["last_review"].fillna(df_airbnb["last_review"].min(), inplace=True)

# Insert "no rule" placeholder in the "house_rules" column for null values
df_airbnb["house_rules"].fillna("no rule", inplace=True)

# Drop rows where the remaining columns has null
df_airbnb.dropna(how='any', inplace=True)

# Check null values again after processing
for col in df_airbnb.columns:
  print(f"{int(int(df_airbnb[col].isnull().sum())/len(df_airbnb[col])*100)}% of total rows; number of rows with null values in {col} is {int(df_airbnb[col].isnull().sum())}")

# Check shape of the DataFrame after handling null values
df_airbnb.shape

"""### **We kept 96% of the original records within `df_airbnb` after null processing.**

# **Part 2: Exploratory Data Analysis (EDA)**

#### **2.1 Understanding Data**

After collectively cleaning the data, we approached individual EDA before collaborating our findings.  Although we previewed the understanding of our dataset in our preprocessing section, we believed it was good practice to further explore and understand our clean data.
"""

# Display the columns in df_airbnb
df_airbnb.columns

# Display the datatypes in df_airbnb
datatypes = df_airbnb.dtypes
datatypes

# Display the descriptive statistics of `df_reservations`
descriptive_statistics = df_airbnb.describe()
descriptive_statistics

"""#### **2.2 Processing and Applying Domain Knowledge**

Upon inspecting our clean data, which exclusively contained Airbnb properties in NYC, we deemed 'country' and 'country_code' as redundant information and removed both columns.  This decision helps reduce unnecessary computational costs, which is particularly beneficial in the context of Big Data.
"""

# Drop 'country' and 'country_code' columns as all records are in the US (no value of keeping these 2 features)
df_airbnb.drop(columns=['country', 'country_code'], inplace=True)

"""On the topic of location, the geographical coordinates of each property were represented by 'lat' (latitude) and 'long' (longitude). Given that these columns contain precise location data, we opted to maintain their datatype as floats. This decision ensures the accuracy and integrity of the geographic information, which is crucial for any spatial analysis or mapping tasks that may follow in our data processing workflow.

We next observed that 'id' was a raw integer value. Recognizing 'id' as a basic identifier for properties, we converted it, as well as 'host_id', to string values. We also confirmed that 'name', 'host_name', 'neighbourhood_group', 'neighbourhood', and 'house_rules' retained their string format, as these columns contain textual information.
"""

# Convert specified columns to string data type
str_columns = ['id', 'name', 'host_id', 'host_name', 'neighbourhood_group', 'neighbourhood', 'house_rules']
df_airbnb[str_columns] = df_airbnb[str_columns].astype(str)

"""From our descriptive statistics, we noted that 'construction_year' was a float value and 'last_review' was stored as datetime64[ns]. We made the decision to standardize these data formats:

- We converted 'construction_year' to a datetime format

- We updated 'last_review' to datetime format and extracted the date
"""

# Convert 'construction_year' to datetime format
df_airbnb['construction_year'] = pd.to_datetime(df_airbnb['construction_year'], format='%Y').dt.year

# Convert 'last_review' to datetime format and extract date
df_airbnb['last_review'] = pd.to_datetime(df_airbnb['last_review']).dt.date

"""In order to streamline our dataset and ensure data consistency, we converted several columns to integer datatype. These columns—'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'review_rate_number', 'calculated_host_listings_count', and 'availability_365'—were specifically chosen because their values represent discrete quantities that are more logically handled as integers. This conversion was implemented using the astype(int) function, which efficiently casts the selected columns of our DataFrame to integer datatype, thereby enhancing data handling and computation efficiency."""

# Convert specified columns to integer
int_columns = ['minimum_nights', 'number_of_reviews', 'reviews_per_month', 'review_rate_number',\
               'calculated_host_listings_count', 'availability_365']
df_airbnb[int_columns] = df_airbnb[int_columns].astype(int)

"""In our dataset, the 'price' and 'service_fee' columns were formatted as strings containing monetary values, which included dollar signs and commas. To facilitate numerical analysis, it was necessary to convert these columns into integers. We achieved this by first removing the dollar signs and commas from each string using the str.replace() method. Following the removal of these characters, we used the pd.to_numeric() function to convert the cleaned strings into numerical data. This transformation allows for more straightforward and efficient computations involving these monetary values in our subsequent data analysis steps."""

# Convert 'price' and 'service_fee' string to int by removing $ and ,
df_airbnb['price'] = pd.to_numeric(df_airbnb['price'].str.replace('$', '').str.replace(',', ''))
df_airbnb['service_fee'] = pd.to_numeric(df_airbnb['service_fee'].str.replace('$', '').str.replace(',', ''))

"""We encoded 'instant_bookable', host_identity_verified', and 'last review' for easier data processing, especially when building our models."""

# Encode 'instant_bookable' column
df_airbnb['instant_bookable'] = df_airbnb['instant_bookable'].map({False: 0, True: 1})

# Encode 'host_identity_verified' column
df_airbnb['host_identity_verified'] = df_airbnb['host_identity_verified'].map({'unconfirmed': 0, 'verified': 1})

# Encode last review
df_airbnb['last_review'] = df_airbnb['last_review'].apply(lambda x: 0 if x <= dt.date(2021, 5, 21) else 1)

# Inspect results after processing each column
df_airbnb.head(2)

"""---------------------------------------------

## **2.3 Visualizations**

### **2.3.1 Geolocation**

To enrich our dataset and increase the precision of our analysis, we incorporated zip code information, which was initially absent, by utilizing geolocation data. We created a function named get_zipcode, which employs the uszipcode search method to derive zip codes from the latitude and longitude coordinates of each property.
"""

# import searchEngine from uszipcode and create a searchEngine object.
from uszipcode import SearchEngine
search = SearchEngine()

"""This function was systematically applied to each row of our DataFrame using the apply method, effectively adding a new 'zipcode' column."""

#zipcode search function for apply lambda function
def get_zipcode(lat, long):
    result = search.by_coordinates(lat = lat, lng = long, returns = 1)
    return result[0].zipcode

#add zipcode column with apply function on each row
df_airbnb['zipcode'] = df_airbnb.apply(lambda x: get_zipcode(x.lat,x.long), axis=1)

"""Given that the application of get_zipcode extended data processing times to over 30 minutes, we opted to save our enhanced dataset as airbnb_clean.csv. This allows us to preserve our progress and facilitates continued analysis without the need to repeat the zip code extraction process in future sessions."""

# Since getting the zipcode may take around 30 mins to run, we may want to save the csv file to reuse in the future.

df_airbnb.to_csv('/content/drive/My Drive/airbnb_clean.csv', index=False)

unique_zipcodes_count = df_airbnb['zipcode'].nunique()
unique_zipcodes_count

"""To further refine our analysis and enhance our visual representations, we aggregated the data by zip code. This step involved creating a new DataFrame, df_zip_graph, where we calculated several statistics per zip code, including average price, maximum price, property count, average review rate, and average number of reviews per month. This grouping allowed us to examine trends and outliers at a more localized level."""

# Then we could create an aggregated dataframe that groups by zipcode to make our visualizations

df_zip_graph = df_airbnb.groupby("zipcode", as_index=False).agg(
    average_price=("price", "mean"),
    max_price = ("price", "max"),
    property_count = ("id", "count"),
    average_review = ("review_rate_number", "mean"),
    average_number_reviews_per_month = ("reviews_per_month", "mean")
)

# Look at the statistics of our data after grouping with zipcode
df_zip_graph.describe()

"""We opened our GeoJSON file containing the geographic information of NYC zip codes, which we used to create a map with Plotly. This map visualizes the property count by zip code, using color gradients to indicate density. This geographic plotting not only provides a visual summary of property distribution but also enhances our ability to communicate these findings effectively."""

# Now we can open up the gejoson file and use it for our plotly graph.
# First graph is property count based on zipcode

with open('/content/nyc-zip-code-tabulation-areas-polygons.geojson') as response:
    nyjson = json.load(response)
fig = px.choropleth(df_zip_graph,
                    geojson=nyjson,
                    locations='zipcode',
                    color='property_count',
                    color_continuous_scale="Viridis",
                    featureidkey="properties.postalCode",
                    range_color=(1,5000),
                    scope="usa",
                    labels={'Total Porperties':'property_count'}
                          )
fig.update_geos(fitbounds="locations")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})

"""Not surprisingly, most of the properties are concentrated in downtown Manhattan and Brooklyn Area.

We next observed average reviews by zipcode.
"""

# Now we look at average review group by zipcode
fig = px.choropleth(df_zip_graph,
                    geojson=nyjson,
                    locations='zipcode',
                    color='average_review',
                    color_continuous_scale="Viridis",
                    featureidkey="properties.postalCode",
                    range_color=(0,5),
                    scope="usa",
                          )
fig.update_geos(fitbounds="locations")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})

"""If just looking at the average review, it's understandable that every zip has similar average. Satisification of the customer doesn't really depends on the neighborhood/ location, but rather the service they receive. This supports the need for additional descriptors for each property that include amenities and number of bedrooms."""

# Now review the average price for the rental property based on zipcode
fig = px.choropleth(df_zip_graph,
                    geojson=nyjson,
                    locations='zipcode',
                    color='average_price',
                    color_continuous_scale="Viridis",
                    featureidkey="properties.postalCode",
                    range_color=(df_zip_graph["average_price"].min(),df_zip_graph["average_price"].max()),
                    scope="usa",
                          )
fig.update_geos(fitbounds="locations")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})

"""***Interesting Finding***: Originally was expecting Manhattan or Brooklyn to have higher average price since it's closer to the metro area. However, this graph did not consider the fact that metro area usually has smaller units (apartment/condo) while locations further away from metro area may have house/ townhouse, which has more space to potentially result in lower price/sqft.  There was a fair even distribution of prices all throughout the city."""

# finally we look at the number of review per month based on zipcode
fig = px.choropleth(df_zip_graph,
                    geojson=nyjson,
                    locations='zipcode',
                    color='average_number_reviews_per_month',
                    color_continuous_scale="Viridis",
                    featureidkey="properties.postalCode",
                    range_color=(df_zip_graph["average_number_reviews_per_month"].min(),df_zip_graph["average_number_reviews_per_month"].max()),
                    scope="usa",
                          )
fig.update_geos(fitbounds="locations")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})

"""***Interesting Finding***: Using average review per month, we can treat this information as turnover rate for guest. The location with highest number of average review is right next to La Guardia Airport, and the second highest one is right next to JFK international airport. It may be worth to consider rental properties in these area, as there are higher turnover rate.

### **2.3.2 Natural Language Processing (NLP)**

Intrigued by the potential applications of NLP and the notable absence of explicit property descriptions (ex. photo images, amenities, and room counts), we decided to harness NLP techniques to extract useful data from the 'house_rules' column. Our aim was to transform unstructured text into structured data that could reveal insights about the properties listed, specifically hoping to highlight amenities and additional descriptors that influence prices.

We started with parsing 'house_rules' for common words that pertain to the property.
"""

nltk.download('stopwords')
nltk.download('punkt')

# Function to clean and split text
def process_text(text):
    # Remove the phrase 'no rule' from text
    text = re.sub(r'\bno rule\b', '', text, flags=re.IGNORECASE)

    # Tokenize text and ensure separation of numbers and texts like '30min' into '30' and 'min'
    tokens = nltk.word_tokenize(text.lower())

    # Extend the separation of alphanumeric compounds like '30min'
    words = []
    for token in tokens:

        # Separate alphanumeric tokens into numbers and text
        separated = re.findall(r'[0-9]+|[a-z]+', token)
        words.extend(separated)

    # Define a custom list of additional words to keep
    additional_keep = ['min']

    # Load stopwords set only once and use it globally
    stop_words = set(stopwords.words('english'))

    # Remove stopwords, except for the words in 'additional_keep'
    words = [word for word in words if word not in stop_words or word in additional_keep]
    return words

# Combine all house rules into one text and process it
all_text = ' '.join(df_airbnb['house_rules'])
words = process_text(all_text)
word_counts = Counter(words)

# Print the top x most common words in 'house_rules'
x = 30
most_common_words = word_counts.most_common(x)

"""We created a word cloud to identify any strong/more frequent words stated in the specified house rules."""

# Convert list of tuples to dictionary for word cloud visual
word_freq = dict(most_common_words)

# Create a WordCloud object
wordcloud = WordCloud(width = 800, height = 400,
                      background_color ='white',
                      max_words=30).generate_from_frequencies(word_freq)

# Plot the WordCloud image
plt.figure(figsize = (8, 6), facecolor = None)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

"""Most properties who have additional specific house rules say 'Please'--a nontrivial finding.  Many mention no smoking. Other popular words are 'pm', 'quiet' 'noise', 'parties', which probably discourage rowdy guests and/or signals a shared accomodation within their house rule.  These were all very generic words to preserve the cleanliness of each property and prevent social disruption.


We proceeded to examine whether a correlation exists between property prices and the presence of designated house rules. The box plot visually contrasts properties without rules against those with rules to determine if a higher price point is associated with the establishment of specific house guidelines.
"""

# Create a binary 'Rule_Status' column where 'No Rule' = 0 and 'Has Rules' = 1
df_airbnb['Rule_Status'] = df_airbnb['house_rules'].apply(lambda x: 0 if x.lower() == 'no rule' else 1)

# Box Plot

palette_colors = {0: "lightblue", 1: "salmon"}

plt.figure(figsize=(8, 6))

sns.boxplot(x='Rule_Status', y='price', data=df_airbnb, hue='Rule_Status', palette=palette_colors, dodge=False)
plt.title('Price Distribution by House Rule Status')
plt.xlabel('Rule Status (0: No Rule, 1: Has Rules)')
plt.ylabel('Price')

plt.legend([],[], frameon=False)

sns.set_style("whitegrid")

plt.show()

"""From this visualization, we can observe that the median prices are quite similar for both categories, although there is a wide range of prices within each. The similar box sizes suggest that the variability in prices is roughly the same whether house rules are specified or not. This implies that the presence of house rules does not significantly affect the price range for listings in our dataset.


We investigated the average pricing of properties in relation to their house rule status. By grouping properties into those without rules and those with rules, we calculated the average price for each category. The resulting bar graph provides a direct comparison, highlighting whether the average price is significantly different between the two groups.
"""

palette_colors = ["lightblue", "salmon"]

# Calculate average prices
average_prices = df_airbnb.groupby('Rule_Status')['price'].mean().reset_index()

plt.figure(figsize=(8, 6))
sns.barplot(x='Rule_Status', y='price', data=average_prices, hue='Rule_Status', palette=palette_colors, dodge=False)
plt.title('Average Price by House Rule Status')
plt.xlabel('Rule Status (0: No Rule, 1: Has Rules)')
plt.ylabel('Average Price')
plt.show()

"""The bar graph illustrates the average price of Airbnb listings differentiated by their house rule status. The lighter blue bar represents properties with 'No Rule' (0), while the salmon-colored bar represents properties that have specified house rules (1). From the graph, there is no significant difference between properties with house rules and proerties without in regard to their average price.

To expand our NLP efforts after tokenizing the 'house_rules', we conducted sentiment analysis using TextBlob to examine the potential influence of sentiment in 'house_rules' on property prices. We assigned a neutral sentiment score of 0.0 to all entries labeled "no rule", ensuring a consistent baseline for comparison.
"""

# Create 'house_rules_sentiment' column and get sentiment score for 'house_rules'.  All rows with "no rule" should have score of 0.0
df_airbnb['house_rules_sentiment'] = df_airbnb['house_rules'].apply(lambda x: TextBlob(x).sentiment.polarity)

df_airbnb.head(2)

"""We looked to see if there was any correlation between the properties' (who had additional rules) house_rules_sentiment versus price.  """

rules_sentiment = df_airbnb[df_airbnb['house_rules'].str.lower() != 'no rule']

plt.figure(figsize=(8, 6))
sns.scatterplot(data=rules_sentiment, x='house_rules_sentiment', y='price')
plt.title('House Rules Sentiment vs Price')
plt.xlabel('House Rules Sentiment Score')
plt.ylabel('Price')
plt.show()

"""Finally, we counted all the words under house_rules to see if there was a correlation with price."""

# Create 'house_rules_word_count' and get word count of 'house_rules'.  All rows with "no rule" will reflect as "0" for its word count
df_airbnb['house_rules_word_count'] = df_airbnb['house_rules'].apply(lambda x: 0 if x.lower() == "no rule" else len(x.split()))

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_airbnb, x='house_rules_word_count', y='price')
plt.title('Price vs. House Rules Word Count')
plt.xlabel('House Rules Word Count')
plt.ylabel('Price')
plt.show()

"""**Ultimately, there were no significant findings in regard to house_rules versus property prices.**

## **2.4 Correlation of Feature Variables**

Including a correlation matrix into our analysis serves several key purposes. It allows us to identify potential relationships between features, which is crucial for effective feature selection and engineering. By revealing insights into the dataset's structure, the matrix informs our modeling decisions and helps prevent issues like multicollinearity.
"""

# Filter DataFrame to include only numeric features
numeric_df = df_airbnb.select_dtypes(include=['number'])
numeric_df = numeric_df.sort_index(axis=1)
numeric_df.columns

# Filter DataFrame to include only text features
text_df = df_airbnb.select_dtypes(exclude=['number'])
text_df = text_df.sort_index(axis=1)
text_df.columns

# Create a correlation matrix using numerics_df and call it corr_mat
corr_mat = numeric_df.corr()
# Using the correlation matrix, generate a correlation heatmap
fig, ax = plt.subplots(figsize = (6, 6))
# Define the date format
sns.heatmap(corr_mat,\
            vmin=-1,
            vmax=1,
            cmap='RdBu',
            ax=ax)
ax.set_title('Correlation Heatmap of Numeric Features');

"""**Here we quickly noticed that the service fee is a percentage of the price. Therefore, we needed to exclude the service fee from the training model**.

# **Part 3: Feature Engineering**

In this section, we aimed to create additional features for our regression models.  Knowing that our dataset lacks information about bedrooms and beds, we sought ways to add these features to our data.
"""

df_airbnb.head()

"""## **3.1 One Hot Encoding**

We one hot encoded cancellation_policy and room_type to be fed into our regression models.
"""

# Perform one-hot encoding for 'cancellation_policy' and 'room_type' columns
df_airbnb = pd.get_dummies(df_airbnb, columns=['cancellation_policy', 'room_type'], dtype=int)

# Clean up column names again because we added dummy columns such as "room_type_Entire home/apt"
df_airbnb.columns = (df_airbnb.columns
                      .str.replace(' ', '_', regex=True)
                      .str.replace('/', '_', regex=True)
                      .str.lower()
                    )
df_airbnb.columns

"""## **3.2 Bedroom Count**

Due to the lack of information about room numbers for each property, we looked to enhance the tokenizing of our process_text function and applied it to our 'name' column.  The aim was to parse bedroom counts for the properties that specified the number of bedrooms in their names, then imputing the average for all the null values.
"""

df_airbnb['parsed_name'] = df_airbnb['name'].apply(process_text)

"""We counted the most common words to help us identify key terms such as 'br', 'studio', '4br' to enhance our process_text fuction as well as create a label_bedrooms function."""

# Create a Counter to find the most common words
all_name_words = [word for sublist in df_airbnb['parsed_name'] for word in sublist]
word_counts = Counter(all_name_words)

# Get the top x most common words and convert the list of tuples [(name_word, count)] to a list of words only
x = 50
most_common_name_words = word_counts.most_common(x)
print(most_common_name_words)

"""This helper function is not 100% accurate, since some properties such as "3-story 2 BR" gets categorized as 3 bedroom."""

def label_bedrooms(parsed_name):

    # Check for specific keywords or numbers and return corresponding bedroom counts
    if 'studio' in parsed_name or '1' in parsed_name:
        return 1
    elif '2' in parsed_name:
        return 2
    elif '3' in parsed_name:
        return 3
    elif '4' in parsed_name:
        return 4
    else:
        return None

df_airbnb['bedroom_count'] = df_airbnb['parsed_name'].apply(label_bedrooms)
df_airbnb['bedroom_count']

# Imputed missing 'bedroom_count' values with the mean
mean_bedroom_count = df_airbnb['bedroom_count'].mean()
df_airbnb['bedroom_count'].fillna(mean_bedroom_count, inplace=True)
df_airbnb['bedroom_count']

df_airbnb['bedroom_count'].describe()

"""----------------

# Part 4: Machine Learning Modeling

Initially, our focus was on predicting nightly prices for Airbnb listings to aid investors in optimizing revenue streams. However, our geospatial analysis uncovered a significant correlation: higher review rates per month were linked to proximity to airports. This led us to rethink our approach.

Inspired by this insight, we refined our modeling strategy. Instead of solely predicting nightly prices, we introduced a new combined target variable: the product of nightly price and average reviews per month. This shift provides a more comprehensive view of revenue potential by considering both pricing and booking frequency.

By integrating pricing and booking frequency targets into our predictive model, as well as engineered input features from our raw dataset, we offer investors insights into listing value and popularity.

## 4.1 Preprocessing: Create Features and Label and Split Data into Train and Test
"""

numeric_df = df_airbnb.select_dtypes(include=['number'])
numeric_df = numeric_df.sort_index(axis=1)
numeric_df.columns

"""We dropped 'construction-year' because the age of a property was not calculated to be included into our model."""

# Store the  target variable into "target"
target = numeric_df['price']*numeric_df["reviews_per_month"]

# Store numeric features dataframe into variable called "features
features = numeric_df.drop(['construction_year', 'price', 'service_fee','reviews_per_month'], axis=1)

# assign appropriate value to seed and conduct 80/20 train-test split with random_state = seed
seed = 42

X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""## 4.2 PCA to Reduce Dimensionality

By reducing multicollinearity, PCA enhances the stability of regularized regression models.
"""

# Intermediate step to address fact that PCA is not scale invariant
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import numpy as np

scaler = StandardScaler()

# Standardizing the features based on unit variance
X_train_normalized = scaler.fit_transform(X_train)

# Instantiate and Fit PCA
pca = PCA(n_components=len(X_train.columns))
pca.fit(X_train_normalized)

# Save the explained variance ratios into variable called "explained_variance_ratios"
explained_variance_ratios = pca.explained_variance_ratio_

# Save the CUMULATIVE explained variance ratios into variable called "cum_evr"
cum_evr = np.cumsum(pca.explained_variance_ratio_)

# find optimal num components to use (n) by plotting explained variance ratio
fig, ax = plt.subplots()
ax.axhline(y=0.9, color='b', linestyle='-')
ax.plot(cum_evr)
ax.set_title('Explained variance ratio given number of components')
ax.set_xlabel('Number of components')
ax.set_ylabel('Explained variance ratio')
ax.set_xticks([x for x in range(0,23)])
ax.set_xticklabels(labels = [x for x in range(1,24)], rotation=0)
plt.show()

# Refit and transform on training with parameter n (as deduced from the last step)
pca = PCA(n_components=18)
X_train_pca = pca.fit_transform(X_train_normalized)

# Transform on Testing Set and store it as `X_test_pca`
X_test_normalized = scaler.transform(X_test)
X_test_pca = pca.transform(X_test_normalized)

"""## 4.3 Regression Models

### (a) Linear Regression with PCA (Unregularized)

We utilized PCAed training data to mitigate potential multicollinearity.
"""

# Import required libraries
from sklearn.linear_model import LinearRegression, ElasticNet
from sklearn.metrics import r2_score

# Initialize model with default parameters and fit it on the training set
reg = LinearRegression()
reg.fit(X_train_pca, y_train)

# Use the model to predict on the test set and save these predictions as `y_pred`
y_pred = reg.predict(X_test_pca)

# Find the R-squared score and store the value in `lin_reg_score`
lin_reg_score = r2_score(y_test,y_pred)
lin_reg_score

"""### (b) Ridge Regression (L2 Regularized)

We utilized PCA-transformed training data, as regularized regression models are not scale-invariant.
"""

# Import required libraries
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for alpha values to tune
param_grid = {'alpha': [0.1, 1, 10, 100]}

# Initialize Ridge model
reg_ridge = Ridge()

# Initialize GridSearchCV with the Ridge model and parameter grid
grid_search = GridSearchCV(estimator=reg_ridge, param_grid=param_grid, cv=5)

# Fit the GridSearchCV on the training data
grid_search.fit(X_train_pca, y_train)

# Get the best model found by GridSearchCV
best_ridge = grid_search.best_estimator_

# Use the best model to predict on the test pca set
y_pred = best_ridge.predict(X_test_pca)

# Find the r2 score and store the value in `ridge_score`
ridge_score = r2_score(y_test,y_pred)
ridge_score

"""### (c) Elastic Net Regression (L1 + L2 Regularized)"""

# Define the parameter grid for alpha and l1_ratio values to tune
param_grid = {'alpha': [0.1, 1, 10, 100],
              'l1_ratio': [0.1, 0.5, 0.9]}

# Initialize model
elastic_net = ElasticNet()

# Initialize GridSearchCV with the ElasticNet model and parameter grid
grid_search = GridSearchCV(estimator=elastic_net, param_grid=param_grid, cv=5)

# Fit the GridSearchCV object on the training data
grid_search.fit(X_train_pca, y_train)

# Get the best model found by GridSearchCV
best_elastic_net = grid_search.best_estimator_

# Use the best model to predict on the test set
y_pred = best_elastic_net.predict(X_test_pca)

# Calculate the r2 score
en_score = r2_score(y_test, y_pred)
en_score

"""### (d) Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor

# prep for parameter tuning
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
    }
# Initialize model
random_forest = RandomForestRegressor(random_state=42)

# Initialize GridSearchCV with the RandomForestRegressor model and parameter grid
grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5)

# Fit the GridSearchCV object on the training data
grid_search.fit(X_train, y_train)

# Get the best model found by GridSearchCV
best_random_forest = grid_search.best_estimator_

# Use the best model to predict on the test set
y_pred = best_random_forest.predict(X_test)

# Calculate the R-squared score
rfr_score = r2_score(y_test, y_pred)
rfr_score

"""# Part 5: Challenges/Potential Next Steps/Future Direction

To conclude our analysis, we have witnessed a notable improvement in the performance of our predictive models following a critical revision of our target variable. Initially, our models struggled to achieve accuracy, with predictions hovering around a mere 2% when solely predicting nightly rates (price). However, with the introduction of our new combined target variable— multiplying price by reviews_per_month— we observed a significant enhancement in predictive accuracy.

Linear Regression, Ridge Regression, and Elastic Net all experienced substantial performance gains, with accuracy levels rising to 27%. While this improvement is notable compared to the initial 2%, it falls short of our ideal accuracy. Notably, our Random Forest Regression (RFR) model demonstrated exceptional improvement, with accuracy soaring from 30% to 62%.

Given the substantial improvement in performance and the superior accuracy achieved by RFR, we have identified it as the most promising model for future iterations of our predictive modeling efforts. While our ultimate goal may be an accuracy of 80%, RFR has proven to be the most effective approach thus far.

Moving forward, we will prioritize the continued refinement and enhancement of our Random Forest Regression model, leveraging further feature engineering, exploring additional data sources, and optimizing model parameters. Additionally, integrating external datasets such as real estate market trends from Zillow or demographic data holds promise for further improving predictive accuracy.

Moreover, as data volume and model complexity increase, enhancing computational efficiency through techniques like parallel processing or cloud-based computation will be crucial.

In conclusion, by addressing these challenges comprehensively and leveraging advanced modeling techniques and technologies, we will be poised to significantly enhance the predictive accuracy of our models. Our concerted effort will provide reliable pricing strategies for Airbnb investors, ensuring that our future models are not only more accurate but also more robust and capable of handling diverse and complex real-world scenarios.
"""